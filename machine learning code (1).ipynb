{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedcbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "import numpy as np\n",
    "\n",
    "# Example data (replace with your actual dataset)\n",
    "X_train = np.random.rand(100, 30)\n",
    "y_train = np.random.randint(0, 2, 100)\n",
    "X_val = np.random.rand(20, 30)\n",
    "y_val = np.random.randint(0, 2, 20)\n",
    "\n",
    "# Build the neural network with regularization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(30,), kernel_regularizer=L2(0.01)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=L1(0.01)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# Function to plot training and validation metrics\n",
    "def plot_metrics(history, model_name):\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics for the trained model\n",
    "plot_metrics(history, \"Baseline Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945832ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Add Dropout layers\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(30,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import L1, L2\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# Build neural network with regularization using Input layer\n",
    "model_regularized = Sequential([\n",
    "    Input(shape=(30,)),  # Define input shape using Input layer\n",
    "    Dense(64, activation='relu', kernel_regularizer=L2(0.01)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=L1(0.01)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train the regularized model\n",
    "model_regularized.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                          loss=BinaryCrossentropy(),\n",
    "                          metrics=[Accuracy()])\n",
    "\n",
    "history_regularized = model_regularized.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Now you can plot the metrics for the regularized model\n",
    "plot_metrics(history_regularized, \"Regularized Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fac63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Input\n",
    "\n",
    "# Build neural network with Dropout layers and Input layer\n",
    "model_dropout = Sequential([\n",
    "    Input(shape=(30,)),  # Define input shape using Input layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train the dropout model\n",
    "model_dropout.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss=BinaryCrossentropy(),\n",
    "                      metrics=[Accuracy()])\n",
    "\n",
    "history_dropout = model_dropout.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Now you can plot the metrics for the dropout model\n",
    "plot_metrics(history_dropout, \"Dropout Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce21026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load and preprocess data\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build neural network with L2 Regularization\n",
    "model_l2 = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(30,), kernel_regularizer=l2(0.01)),  # Apply L2 Regularization\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_l2.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss=BinaryCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_l2 = model_l2.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plot training and validation metrics\n",
    "def plot_metrics(history, model_name):\n",
    "    # Loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title(f'{model_name} - Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='green')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "    plt.title(f'{model_name} - Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(history_l2, \"L2 Regularization Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example accuracy values for different models\n",
    "accuracy_values = [96.5, 97.8, 98.1]  # Baseline, L2 Regularization, Dropout\n",
    "labels = ['Baseline', 'L2 Regularization', 'Dropout Regularization']\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(accuracy_values, labels=labels, autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Accuracy Comparison of Regularization Techniques')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ee3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Install scikeras if you haven't already\n",
    "!pip install scikeras  # This line installs the necessary package\n",
    "# Instead of tensorflow.keras.wrappers.scikit_learn, import from scikeras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Evaluate feature importance using permutation importance\n",
    "# The model must first be compatible with sklearn, so we use KerasClassifier\n",
    "\n",
    "\n",
    "# Wrap the Keras model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(30,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss=BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "model_for_sklearn = KerasClassifier(build_fn=create_model, epochs=50, batch_size=32, verbose=0)\n",
    "model_for_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Compute permutation importance\n",
    "perm_importance = permutation_importance(model_for_sklearn, X_test, y_test, scoring=\"accuracy\", n_repeats=10, random_state=42)\n",
    "\n",
    "# Organize feature importance into a DataFrame\n",
    "feature_names = data.feature_names\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(perm_importance_df['Feature'], perm_importance_df['Importance'], color='purple')\n",
    "plt.title('Feature Importance (Dropout Regularized Neural Network)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
